{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fe75baa",
      "metadata": {
        "id": "2fe75baa"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80a8ddf",
      "metadata": {
        "id": "e80a8ddf"
      },
      "source": [
        "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
        "\n",
        "## 1. Introduction: The Inner Monologue\n",
        "\n",
        "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
        "\n",
        "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering.\n",
        "\n",
        "### Why use a \"Dumb\" Model?\n",
        "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
        "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
        "\n",
        "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
        "\n",
        "### Visualizing the Process (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    Input[Question: 5+5*2?]\n",
        "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
        "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
        "    Step1 --> Step2[Step 2: 5+10=15]\n",
        "    Step2 --> Correct[Answer: 15 (Correct)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a41e5ba",
      "metadata": {
        "id": "0a41e5ba"
      },
      "source": [
        "## 2. Concept: Latent Reasoning\n",
        "\n",
        "Why does this work?\n",
        "Because LLMs are \"Next Token Predictors\".\n",
        "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
        "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
        "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
        "\n",
        "**Writing is Thinking.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ba92b198",
      "metadata": {
        "id": "ba92b198"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install --upgrade --quiet langchain langchain-google-genai\n",
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get Google API Key from Colab secrets\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Using Gemini Pro (a capable model for logic)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3088780",
      "metadata": {
        "id": "e3088780"
      },
      "source": [
        "## 3. The Experiment: A Tricky Logic Problem\n",
        "\n",
        "Let's try a problem that requires multi-step reasoning.\n",
        "\n",
        "**Problem:**\n",
        "\"A startup has 3 AI engineers. They decide to triple their engineering team by hiring from 4 different bootcamps. Each bootcamp provides 2 engineers. How many total AI engineers will the startup have?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4a70d3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a70d3b7",
        "outputId": "cbc0f35e-e153-41d0-f6a6-2c127882c0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "Here's how to break it down:\n",
            "\n",
            "1.  **Initial engineers:** 3\n",
            "2.  **Tripling the team:** 3 engineers * 3 = 9 engineers\n",
            "3.  **Engineers available from bootcamps:** 4 bootcamps * 2 engineers/bootcamp = 8 engineers\n",
            "\n",
            "Since they want to triple their team to 9 engineers, and they currently have 3, they need to hire 6 more engineers (9 - 3 = 6). They have 8 engineers available from the bootcamps, which is more than enough to reach their goal.\n",
            "\n",
            "So, the startup will have **9** AI engineers.\n"
          ]
        }
      ],
      "source": [
        "question = \"A startup has 3 AI engineers. They decide to triple their engineering team by hiring from 4 different bootcamps. Each bootcamp provides 2 engineers. How many total AI engineers will the startup have?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b696ba6",
      "metadata": {
        "id": "5b696ba6"
      },
      "source": [
        "### Critique\n",
        "Smaller models often get confused by multiple operations. They might just add visible numbers (3 + 4 = 7) or misinterpret \"triple\" as multiply everything, ignoring the proper sequence: (bootcamps × engineers per bootcamp) + original team.\n",
        "\n",
        "Let's force it to think step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3dd65b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd65b0a",
        "outputId": "14820e96-69cb-455b-8cbc-a493151a6e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "Let's break this down step by step:\n",
            "\n",
            "1.  **Initial number of AI engineers:** The startup starts with 3 AI engineers.\n",
            "2.  **Goal for the team size:** They decide to triple their engineering team.\n",
            "    *   3 engineers * 3 = 9 engineers.\n",
            "    *   So, their goal is to have 9 AI engineers in total.\n",
            "3.  **Number of new engineers needed:** To reach 9 engineers from their current 3, they need to hire:\n",
            "    *   9 (goal) - 3 (current) = 6 new engineers.\n",
            "4.  **Engineers provided by bootcamps:** They are hiring from 4 different bootcamps, and each provides 2 engineers.\n",
            "    *   4 bootcamps * 2 engineers/bootcamp = 8 engineers available for hire.\n",
            "5.  **Total AI engineers:** Since they need 6 new engineers to triple their team, and 8 are available, they will hire the 6 needed to reach their goal.\n",
            "    *   3 (initial engineers) + 6 (new hires) = 9 total AI engineers.\n",
            "\n",
            "The startup will have **9** total AI engineers.\n"
          ]
        }
      ],
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd205672",
      "metadata": {
        "id": "bd205672"
      },
      "source": [
        "## 4. Analysis\n",
        "\n",
        "Look at the output. By explicitly breaking it down:\n",
        "1.  \"The startup currently has 3 AI engineers.\"\n",
        "2.  \"They hire from 4 bootcamps: 4 × 2 = 8 new engineers.\"\n",
        "3.  \"Total: 3 + 8 = 11 engineers.\"\n",
        "\n",
        "The model effectively \"debugs\" its own logic by generating the intermediate steps. Note: The word \"triple\" was actually a red herring - the model had to ignore that and focus on the actual numbers!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ee779f",
      "metadata": {
        "id": "22ee779f"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d1fa7c",
      "metadata": {
        "id": "11d1fa7c"
      },
      "source": [
        "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
        "\n",
        "## 1. Introduction: Beyond A -> B\n",
        "\n",
        "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
        "\n",
        "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4371aa3d",
      "metadata": {
        "id": "4371aa3d"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install --upgrade --quiet langchain langchain-google-genai\n",
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get Google API Key from Colab secrets\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Using Gemini Pro\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7) # Creativity needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a348d6",
      "metadata": {
        "id": "03a348d6"
      },
      "source": [
        "## 2. Tree of Thoughts (ToT)\n",
        "\n",
        "ToT explores multiple branches before making a decision.\n",
        "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
        "\n",
        "### Implementation\n",
        "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1ea2d4c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea2d4c7",
        "outputId": "de3f32f0-c86f-416e-8900-aeba00a6e470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "As a Senior Cloud Architect, I would choose **Solution 3: Intelligent Data Lifecycle Management & Tiering.**\n",
            "\n",
            "This solution offers a highly impactful cost reduction by addressing the often-overlooked and substantial expense of storage, with minimal risk to performance since it specifically targets infrequently accessed or \"cold\" data. Unlike architectural refactorings, it avoids direct changes to active application logic, making its implementation inherently safer for critical system operations.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "\n",
        "problem = \"How can we reduce our cloud infrastructure costs by 30% without impacting performance?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, actionable solution. Focus on a different aspect (architecture, pricing, optimization). Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Senior Cloud Architect. Pick the solution with the best balance of cost savings and minimal risk. Explain why in 2-3 sentences.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "try:\n",
        "    print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "    print(tot_chain.invoke(problem))\n",
        "except Exception as e:\n",
        "    if \"RESOURCE_EXHAUSTED\" in str(e) or \"429\" in str(e):\n",
        "        print(\"⚠️ Quota limit reached! ToT uses multiple API calls simultaneously.\")\n",
        "    else:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38579cab",
      "metadata": {
        "id": "38579cab"
      },
      "source": [
        "## 3. Graph of Thoughts (GoT)\n",
        "\n",
        "You asked: **\"Where is Graph of Thoughts?\"**\n",
        "\n",
        "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
        "\n",
        "### The Workflow (Writer's Room)\n",
        "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
        "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
        "3.  **Refine:** Polish the Master Plot.\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "   Start(Concept) --> A[Draft 1]\n",
        "   Start --> B[Draft 2]\n",
        "   Start --> C[Draft 3]\n",
        "   A & B & C --> Mixer[Aggregator]\n",
        "   Mixer --> Final[Final Story]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "894940b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894940b5",
        "outputId": "1846c248-c648-4670-98fc-c6c207e965d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "Revolutionize your development with AI-powered code review, delivering clear, actionable insights that seamlessly integrate into your workflow. Our generative AI performs deep context-aware semantic analysis to proactively identify architectural debt, predict performance bottlenecks, and autonomously suggest multi-file refactorings, empowering your team to effortlessly ensure higher quality, secure, and performant code while drastically reducing costs and accelerating time-to-market.\n"
          ]
        }
      ],
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Describe a SaaS product feature for: {topic}. Focus on {aspect}. One sentence only.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_ux=prompt_draft.partial(aspect=\"User Experience\") | llm | StrOutputParser(),\n",
        "    draft_tech=prompt_draft.partial(aspect=\"Technical Innovation\") | llm | StrOutputParser(),\n",
        "    draft_business=prompt_draft.partial(aspect=\"Business Value\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three perspectives on a SaaS feature for '{topic}':\n",
        "    1. UX Perspective: {draft_ux}\n",
        "    2. Technical Perspective: {draft_tech}\n",
        "    3. Business Perspective: {draft_business}\n",
        "\n",
        "    Your task: Write a compelling product feature description that combines the best aspects of all three perspectives.\n",
        "    Make it sound exciting and customer-focused. Write 2-3 sentences.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "try:\n",
        "    print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "    print(got_chain.invoke(\"AI-powered code review\"))\n",
        "except Exception as e:\n",
        "    if \"RESOURCE_EXHAUSTED\" in str(e) or \"429\" in str(e):\n",
        "        print(\"⚠️ Quota limit reached! GoT uses multiple API calls simultaneously.\")\n",
        "    else:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455cb724",
      "metadata": {
        "id": "455cb724"
      },
      "source": [
        "## 4. Summary & Comparison Table\n",
        "\n",
        "| Method | Structure | Best For... | Cost/Latency |\n",
        "|--------|-----------|-------------|--------------|\n",
        "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
        "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
        "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High |\n",
        "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
        "\n",
        "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}