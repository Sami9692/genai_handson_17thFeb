{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1zJZfWCSL3"
      },
      "source": [
        "---\n",
        "\n"
      ],
      "id": "8c1zJZfWCSL3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2bERJzbCSL5"
      },
      "source": [
        "# Unit 2 - Part 2a: The Anatomy of a Prompt\n",
        "\n",
        "## 1. Introduction: Stochasticity (Randomness)\n",
        "\n",
        "Why does the AI give different answers? Because it is **Stochastic** (Random).\n",
        "\n",
        "It predicts the NEXT TOKEN based on probability.\n",
        "\n",
        "### Visualizing the Prediction\n",
        "Input: `\"The sky is...\"`\n",
        "\n",
        "| Word | Probability | Selected? (Temp=0) | Selected? (Temp=1) |\n",
        "|------|-------------|--------------------|--------------------|\n",
        "| Blue | 80% | âœ… | âŒ |\n",
        "| Gray | 15% | âŒ | âœ… |\n",
        "| Green| 1% | âŒ | âŒ |\n",
        "\n",
        "Prompt Engineering is the art of **manipulating these probabilities**."
      ],
      "id": "Q2bERJzbCSL5"
    },
    {
      "cell_type": "markdown",
      "id": "91e03d6f",
      "metadata": {
        "id": "91e03d6f"
      },
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "First, let's install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c7af5d75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7af5d75",
        "outputId": "1b639a7b-abb3-4b80-b0f7-cdbe1c3f8e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet python-dotenv langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1ea88321",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea88321",
        "outputId": "6eb716e5-cebd-4263-c731-74bd55213e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "# Using Low Temp for consistent comparison\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b87b7d",
      "metadata": {
        "id": "79b87b7d"
      },
      "source": [
        "## 2. The CO-STAR Framework (simplified)\n",
        "\n",
        "A good prompt usually has:\n",
        "1.  **C**ontext (Who are you? Who acts?)\n",
        "2.  **O**bjective (What is the task?)\n",
        "3.  **S**tyle (Formal? Funny?)\n",
        "4.  **T**one (Empathetic? Direct?)\n",
        "5.  **A**udience (Who is reading this?)\n",
        "6.  **R**esponse Format (JSON? List?)\n",
        "\n",
        "Let's compare a **Lazy Prompt** vs a **CO-STAR Prompt**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d6811389",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6811389",
        "outputId": "905239fd-6374-4635-819d-d8959f643cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LAZY PROMPT ---\n",
            "Subject: Update on Your Senior Machine Learning Engineer Application at [Company Name]\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "Thank you for your interest in the Senior Machine Learning Engineer position at [Company Name] and for taking the time to interview with our team. We truly appreciate you sharing your experience and qualifications with us.\n",
            "\n",
            "We received a large number of highly qualified applicants for this role, and after careful consideration, we have decided to move forward with other candidates whose experience and skills were a closer match for the specific requirements of this position at this time.\n",
            "\n",
            "This was a very competitive search, and we truly appreciate your interest in joining our team. We wish you the very best in your job search and future endeavors.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name/Hiring Manager Name]\n",
            "[Your Title/HR Department]\n",
            "[Company Name]\n"
          ]
        }
      ],
      "source": [
        "# The Task: Reject a candidate for a job.\n",
        "task = \"Write a rejection email to a candidate named Sarah who applied for a Senior Machine Learning Engineer position.\"\n",
        "\n",
        "print(\"--- LAZY PROMPT ---\")\n",
        "print(llm.invoke(task).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f43fe6",
      "metadata": {
        "id": "48f43fe6"
      },
      "source": [
        "## 3. Hallucination vs. Creativity\n",
        "\n",
        "Did the model make up a reason?\n",
        "Since we didn't give it facts, it **Predicted the most likely reason** (Usually \"Experience\" or \"Volume of applications\").\n",
        "\n",
        "**This is NOT a bug.** It is a feature. The model is *completing the pattern* of a rejection email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "727f0d12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "727f0d12",
        "outputId": "5ab08733-1a5a-4cbb-c387-d4821f090f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STRUCTURED PROMPT ---\n",
            "Dear Alex Chen,\n",
            "\n",
            "Thank you for your interest in the Senior ML Engineer role at NeuralNinja. While your qualifications are impressive, the role requirements evolved during our process. We wish you the best in your job search.\n",
            "\n",
            "Keep innovating,\n",
            "NeuralNinja HR\n"
          ]
        }
      ],
      "source": [
        "structured_prompt = \"\"\"\n",
        "# Context\n",
        "You are an HR Manager at a quirky AI startup called 'NeuralNinja' that builds AI tools for developers.\n",
        "\n",
        "# Objective\n",
        "Write a rejection email to a candidate named Alex Chen who applied for a Senior ML Engineer role.\n",
        "\n",
        "# Constraints\n",
        "1. Be extremely brief (under 50 words).\n",
        "2. Do NOT say 'we found someone better'. Say 'the role requirements evolved'.\n",
        "3. Sign off with 'Keep innovating'.\n",
        "\n",
        "# Output Format\n",
        "Plain text, no subject line.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- STRUCTURED PROMPT ---\")\n",
        "print(llm.invoke(structured_prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886fa865",
      "metadata": {
        "id": "886fa865"
      },
      "source": [
        "## 4. Key Takeaway: Ambiguity is the Enemy\n",
        "\n",
        "Every piece of information you leave out is a gap the model MUST fill with probability.\n",
        "- If you don't say \"Be brief\", it picks the most probable length (Avg email length).\n",
        "- If you don't say \"Be rude\", it picks the most probable tone (Polite/Neutral)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3478f89a",
      "metadata": {
        "id": "3478f89a"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "Write a structured prompt to generate a **Python Function**.\n",
        "- **Context:** You are a Senior Python Dev.\n",
        "- **Objective:** Write a function to reverse a string.\n",
        "- **Constraint:** It must use recursion (no slicing `[::-1]`).\n",
        "- **Style:** Include detailed docstrings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "59a807af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59a807af",
        "outputId": "fb5f2e89-7d74-45aa-cc7e-dcb0d3e60f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SOLUTION ---\n",
            "```python\n",
            "def reverse_string_recursive(s: str) -> str:\n",
            "    \"\"\"\n",
            "    Reverses a given string using a recursive approach.\n",
            "\n",
            "    This function takes an input string and returns a new string with its\n",
            "    characters in reverse order. It operates by recursively processing the\n",
            "    substring excluding the first character, and then appending the first\n",
            "    character to the result of that recursive call.\n",
            "\n",
            "    Args:\n",
            "        s (str): The input string to be reversed.\n",
            "\n",
            "    Returns:\n",
            "        str: The reversed string.\n",
            "\n",
            "    Examples:\n",
            "        >>> reverse_string_recursive(\"hello\")\n",
            "        'olleh'\n",
            "        >>> reverse_string_recursive(\"Python\")\n",
            "        'nohtyP'\n",
            "        >>> reverse_string_recursive(\"\")\n",
            "        ''\n",
            "        >>> reverse_string_recursive(\"a\")\n",
            "        'a'\n",
            "        >>> reverse_string_recursive(\"racecar\")\n",
            "        'racecar'\n",
            "    \"\"\"\n",
            "    # Base case: If the string is empty or contains a single character,\n",
            "    # it is already reversed. This handles the empty string edge case\n",
            "    # and the termination condition for recursion.\n",
            "    if len(s) <= 1:\n",
            "        return s\n",
            "    else:\n",
            "        # Recursive step:\n",
            "        # 1. Call the function with the substring starting from the second character (s[1:]).\n",
            "        #    This effectively reverses the \"tail\" of the string.\n",
            "        # 2. Append the first character of the original string (s[0]) to the result\n",
            "        #    of the recursive call. This places the first character at the end.\n",
            "        return reverse_string_recursive(s[1:]) + s[0]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Assignment Solution\n",
        "\n",
        "assignment_prompt = \"\"\"\n",
        "# Context\n",
        "You are a Senior Python Developer at a tech company, known for writing clean, well-documented code.\n",
        "\n",
        "# Objective\n",
        "Write a Python function to reverse a string.\n",
        "\n",
        "# Constraints\n",
        "1. MUST use recursion (no slicing like [::-1]).\n",
        "2. Include comprehensive docstrings with:\n",
        "   - Function description\n",
        "   - Args section\n",
        "   - Returns section\n",
        "   - Example usage\n",
        "3. Add type hints.\n",
        "4. Handle edge cases (empty string).\n",
        "\n",
        "# Style\n",
        "Professional, production-ready code with detailed documentation.\n",
        "\n",
        "# Output Format\n",
        "Only the Python code, no explanations outside the docstring.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- SOLUTION ---\")\n",
        "print(llm.invoke(assignment_prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3053b5",
      "metadata": {
        "id": "5f3053b5"
      },
      "source": [
        "## Solution Explanation\n",
        "\n",
        "**Key CO-STAR Elements Used:**\n",
        "1. **Context:** Established expertise level (Senior Dev)\n",
        "2. **Objective:** Clear task (reverse string)\n",
        "3. **Style:** Professional, production-ready\n",
        "4. **Constraints:** Specific technical requirements (recursion, type hints, edge cases)\n",
        "5. **Response Format:** Code only, no extra text\n",
        "\n",
        "This structured approach ensures the LLM generates exactly what we need!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318d23f4",
      "metadata": {
        "id": "318d23f4"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd553194",
      "metadata": {
        "id": "fd553194"
      },
      "source": [
        "# Unit 2 - Part 2b: Zero-Shot to Few-Shot\n",
        "\n",
        "## 1. Introduction: In-Context Learning\n",
        "\n",
        "How does the model learn without training?\n",
        "This is called **In-Context Learning**.\n",
        "\n",
        "### The Attention Mechanism (Flowchart)\n",
        "When you ask a question, the model \"looks back\" at the previous text to find patterns.\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    Input[Current Input: 'Angry + Hungry'] -->|Attention Query| History\n",
        "    subgraph History [The Prompt Examples]\n",
        "        Ex1[Ex1: Breakfast + Lunch = Brunch]\n",
        "        Ex2[Ex2: Chill + Relax = Chillax]\n",
        "    end\n",
        "    History -->|Pattern Found: Mix words & define| Prediction[Output: Hangry]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "02843dde",
      "metadata": {
        "id": "02843dde"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b5d34d",
      "metadata": {
        "id": "70b5d34d"
      },
      "source": [
        "## 2. Zero-Shot (No Context)\n",
        "\n",
        "The model relies purely on its training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d7781341",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7781341",
        "outputId": "b0863876-a26f-4349-d9bd-a497a2285d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot: Here are a few creative portmanteaus combining 'Work' and 'Vacation', each with a definition:\n",
            "\n",
            "1.  **Workcation**\n",
            "    *   **Definition:** A trip where an individual travels to a new location, often a leisure destination, and combines their professional work responsibilities with elements of a vacation, such as exploring the area, relaxing, or enjoying local amenities, typically while working remotely.\n",
            "    *   *Example:* \"After a stressful quarter, Sarah booked a two-week **workcation** in Bali, spending mornings on her laptop and afternoons surfing.\"\n",
            "\n",
            "2.  **Bliss-ness**\n",
            "    *   **Definition:** The harmonious integration of professional duties with personal enjoyment and relaxation, typically experienced when working remotely from a desirable vacation location, aiming for a state of productive contentment.\n",
            "    *   *Example:* \"Our team's new remote policy encourages **bliss-ness**, letting us find our perfect balance between deadlines and downtime.\"\n",
            "\n",
            "3.  **Taskation**\n",
            "    *   **Definition:** A vacation where specific, pre-planned work tasks are completed, allowing for a change of scenery and some leisure time, but with a clear commitment to getting essential work done without completely disconnecting.\n",
            "    *   *Example:* \"I'm on a **taskation** this week; I'm at the cabin, but I've blocked out 9-12 daily to finish that report.\"\n",
            "\n",
            "4.  **Wacation**\n",
            "    *   **Definition:** A playful term for a vacation that unexpectedly (or intentionally) includes a significant amount of work, often blurring the lines between leisure and professional responsibilities in a way that might be slightly less structured than a typical \"workcation.\"\n",
            "    *   *Example:* \"My 'relaxing getaway' turned into a full-blown **wacation** when a client emergency popped up.\"\n"
          ]
        }
      ],
      "source": [
        "prompt_zero = \"Combine 'Work' and 'Vacation' into a creative portmanteau word and define it.\"\n",
        "print(f\"Zero-Shot: {llm.invoke(prompt_zero).content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6c1820",
      "metadata": {
        "id": "9b6c1820"
      },
      "source": [
        "## 3. Few-Shot (Pattern Matching)\n",
        "\n",
        "We provide examples. The Attention Mechanism attends to the **Structure** (`Input -> Output`) and the **Tone** (Sarcasm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "832f1788",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "832f1788",
        "outputId": "79f43ec1-63c9-4369-898b-abb57229d5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-Shot: Workation (The modern delusion that you can escape the office by just moving your laptop to a beach.)\n"
          ]
        }
      ],
      "source": [
        "prompt_few = \"\"\"\n",
        "Combine words into a creative portmanteau. Give a witty, sarcastic definition.\n",
        "\n",
        "Input: Breakfast + Lunch\n",
        "Output: Brunch (An excuse to drink mimosas at 11 AM and call it 'cultured')\n",
        "\n",
        "Input: Smoke + Fog\n",
        "Output: Smog (Nature's way of punishing cities for having too many cars)\n",
        "\n",
        "Input: Glamour + Camping\n",
        "Output: Glamping (Pretending you love nature while sleeping in a $500/night tent with WiFi)\n",
        "\n",
        "Input: Work + Vacation\n",
        "Output:\n",
        "\"\"\"\n",
        "print(f\"Few-Shot: {llm.invoke(prompt_few).content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306a3c66",
      "metadata": {
        "id": "306a3c66"
      },
      "source": [
        "## 4. Critical Analysis\n",
        "\n",
        "If you provide **bad examples**, the model will learn the **bad pattern**.\n",
        "This is why Data Quality in your prompt is just as important as code quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0583ce42",
      "metadata": {
        "id": "0583ce42"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad369bc1",
      "metadata": {
        "id": "ad369bc1"
      },
      "source": [
        "# Unit 2 - Part 2c: Advanced Templates & Theory\n",
        "\n",
        "## 1. Theory: Engineering vs. Training\n",
        "\n",
        "### Hard Prompts (Prompt Engineering)\n",
        "- **What:** You change the text input.\n",
        "- **Cost:** Cheap, fast, easy to iterate.\n",
        "- **Use Case:** Prototyping, General tasks.\n",
        "\n",
        "### Soft Prompts (Fine Tuning)\n",
        "- **What:** You change the model's internal weights (mathematically).\n",
        "- **Cost:** Expensive, slow, needs data.\n",
        "- **Use Case:** Domain specificity (Medical, Legal), Behavioral change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2ec94769",
      "metadata": {
        "id": "2ec94769"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e03e7b",
      "metadata": {
        "id": "b1e03e7b"
      },
      "source": [
        "## 2. Dynamic Few-Shotting\n",
        "\n",
        "If you have 1000 examples, you can't fit them all in the context window.\n",
        "We use a **Selector** to pick the best ones.\n",
        "\n",
        "### The Selector Flow (Flowchart)\n",
        "```mermaid\n",
        "graph LR\n",
        "    Input[User Input] -->|Semantic Search| Database[Example Database]\n",
        "    Database -->|Top 3 Matches| Selector\n",
        "    Selector -->|Inject| Prompt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "30c20758",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30c20758",
        "outputId": "5c086b21-a50f-4a77-b2cc-e729f0154085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: This meeting was a waste of time.\n",
            "Output: I'm keen to ensure our future collaborative sessions are structured for optimal efficiency and actionable outcomes.\n",
            "\n",
            "Input: The design looks terrible.\n",
            "Output: The current visual aesthetic presents opportunities for refinement.\n",
            "\n",
            "Input: We're way over budget.\n",
            "Output: Our financial resource utilization requires optimization.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "\n",
        "# 1. Our Database of Examples\n",
        "examples = [\n",
        "    {\"input\": \"The server crashed again.\", \"output\": \"We are experiencing an unplanned service interruption.\"},\n",
        "    {\"input\": \"This code is garbage.\", \"output\": \"The current implementation has room for optimization.\"},\n",
        "    {\"input\": \"The deadline is impossible.\", \"output\": \"The timeline presents ambitious delivery expectations.\"},\n",
        "    {\"input\": \"Nobody uses this feature.\", \"output\": \"This feature shows minimal user engagement metrics.\"},\n",
        "]\n",
        "\n",
        "# 2. Template for ONE example\n",
        "example_fmt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\")\n",
        "])\n",
        "\n",
        "# 3. The Few-Shot Container\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_fmt,\n",
        "    examples=examples\n",
        ")\n",
        "\n",
        "# 4. The Final Chain\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a Corporate-Speak Translator for tech companies. Transform blunt statements into diplomatic, professional language.\"),\n",
        "    few_shot_prompt,      # Inject examples here\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "chain = final_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Test with multiple examples (with rate limiting and error handling)\n",
        "test_inputs = [\n",
        "    \"This meeting was a waste of time.\",\n",
        "    \"The design looks terrible.\",\n",
        "    \"We're way over budget.\"\n",
        "]\n",
        "\n",
        "for i, test_input in enumerate(test_inputs):\n",
        "    try:\n",
        "        print(f\"Input: {test_input}\")\n",
        "        print(f\"Output: {chain.invoke({'text': test_input})}\")\n",
        "        print()\n",
        "\n",
        "        # Add delay between requests to avoid rate limits (except for last one)\n",
        "        if i < len(test_inputs) - 1:\n",
        "            time.sleep(2)  # Wait 2 seconds between requests\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"RESOURCE_EXHAUSTED\" in str(e) or \"429\" in str(e):\n",
        "            print(f\"âš ï¸ Quota limit reached! You've hit your daily API limit.\")\n",
        "            print(f\"ðŸ’¡ Wait for your quota to reset or upgrade your plan.\")\n",
        "            print(f\"ðŸ”— Check usage: https://ai.dev/rate-limit\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgBXjR5kCSL9"
      },
      "source": [
        "## 3. Analysis\n",
        "\n",
        "Using `FewShotChatMessagePromptTemplate` creates a clean separation between instructions and data. This helps the Attention Mechanism focus on the right things."
      ],
      "id": "VgBXjR5kCSL9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}